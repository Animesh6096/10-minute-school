#!/usr/bin/env python3
"""
Enhanced Story-Focused Document Processor for Bengali Literature
Fixes Bengali encoding issues and focuses on extracting actual story content
"""

import os
import re
import json
import unicodedata
from typing import List, Dict, Tuple, Optional
import fitz  # PyMuPDF for PDF processing
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class BengaliTextProcessor:
    """Advanced Bengali text processor with proper Unicode handling"""
    
    @staticmethod
    def fix_bengali_encoding(text: str) -> str:
        """Fix broken Bengali text encoding comprehensively"""
        
        # Step 1: Normalize to NFC (Canonical Decomposition + Composition)
        text = unicodedata.normalize('NFC', text)
        
        # Step 2: Fix common broken conjuncts and characters
        # These are the actual broken patterns we see in the PDF
        conjunct_fixes = {
            # Fix рж░рзН + ржЕржирзНржп ржЕржХрзНрж╖рж░ (рж░рзН followed by other characters)
            'рж░рзНрж┐': 'рж░рж┐',      # рж░рзНрж┐ -> рж░рж┐  
            'рж░рзНржмрзН': 'рж░рзНржм',      # рж░рзНржмрзН -> рж░рзНржм (remove extra halant)
            'рж░рзНржи': 'рж░ржи',      # рж░рзНржи -> рж░ржи
            'рж░рзНржд': 'рж░ржд',      # рж░рзНржд -> рж░ржд
            'рж░рзНржЪ': 'рж░ржЪ',      # рж░рзНржЪ -> рж░ржЪ
            'рж░рзНржХ': 'рж░ржХ',      # рж░рзНржХ -> рж░ржХ
            'рж░рзНржо': 'рж░ржо',      # рж░рзНржо -> рж░ржо
            'рж░рзНржк': 'рж░ржк',      # рж░рзНржк -> рж░ржк
            'рж░рзНрж▓': 'рж░рж▓',      # рж░рзНрж▓ -> рж░рж▓
            'рж░рзНрж╕': 'рж░рж╕',      # рж░рзНрж╕ -> рж░рж╕
            'рж░рзНржЧ': 'рж░ржЧ',      # рж░рзНржЧ -> рж░ржЧ
            'рж░рзНрже': 'рж░рже',      # рж░рзНрже -> рж░рже
            'рж░рзНржн': 'рж░ржн',      # рж░рзНржн -> рж░ржн
            'рж░рзНржж': 'рж░ржж',      # рж░рзНржж -> рж░ржж
            'рж░рзНржЬ': 'рж░ржЬ',      # рж░рзНржЬ -> рж░ржЬ
            'рж░рзНржп': 'рж░рзНржп',      # Keep as is (this is correct)
            
            # Fix ржирзН + ржЕржирзНржп ржЕржХрзНрж╖рж░ patterns  
            'ржирзНржд': 'ржирзНржд',     # Keep correct
            'ржирзНржз': 'ржирзНржз',     # Keep correct
            'ржирзНржж': 'ржирзНржж',     # Keep correct
            'ржирзНржи': 'ржирзНржи',     # Keep correct
            
            # Fix рждрзН + ржЕржирзНржп ржЕржХрзНрж╖рж░ patterns
            'рждрзНржд': 'рждрзНржд',     # Keep correct
            'рждрзНрж░': 'рждрзНрж░',     # Keep correct
            'рждрзНржо': 'рждрзНржо',     # Keep correct
            
            # Fix ржХрзН + ржЕржирзНржп ржЕржХрзНрж╖рж░ patterns
            'ржХрзНржд': 'ржХрзНржд',     # Keep correct
            'ржХрзНрж░': 'ржХрзНрж░',     # Keep correct
            'ржХрзНрж╖': 'ржХрзНрж╖',     # Keep correct
            
            # Fix рж▓рзН + ржЕржирзНржп ржЕржХрзНрж╖рж░ patterns
            'рж▓рзНрж▓': 'рж▓рзНрж▓',     # Keep correct
            'рж▓рзНржк': 'рж▓рзНржк',     # Keep correct
            'рж▓рзНржЯ': 'рж▓рзНржЯ',     # Keep correct
            
            # Fix рж╕рзН + ржЕржирзНржп ржЕржХрзНрж╖рж░ patterns
            'рж╕рзНржд': 'рж╕рзНржд',     # Keep correct
            'рж╕рзНрже': 'рж╕рзНрже',     # Keep correct
            'рж╕рзНржм': 'рж╕рзНржм',     # Keep correct
            'рж╕рзНржо': 'рж╕рзНржо',     # Keep correct
            'рж╕рзНржи': 'рж╕рзНржи',     # Keep correct
            
            # Clean up zero-width characters
            '\u200c': '',     # Remove ZWNJ (Zero Width Non-Joiner)
            '\u200d': '',     # Remove ZWJ (Zero Width Joiner)
            '\ufeff': '',     # Remove BOM (Byte Order Mark)
        }
        
        # Apply fixes
        for broken, fixed in conjunct_fixes.items():
            text = text.replace(broken, fixed)
        
        # Step 3: Fix specific Bengali character issues
        text = re.sub(r'рзН([ржХ-рж╣ржбрж╝ржврж╝ржпрж╝])', r'\1', text)  # Remove unnecessary halant before consonants
        
        # Step 4: Normalize whitespace
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        return text
    
    @staticmethod
    def is_story_content(text: str) -> bool:
        """Determine if text is likely story content vs MCQ/metadata"""
        # Story content indicators (including train scene indicators)
        story_indicators = [
            'ржЧрж▓рзНржк', 'ржХрж╛рж╣рж┐ржирзА', 'ржЪрж░рж┐рждрзНрж░', 'ржирж╛ржпрж╝ржХ', 'ржирж╛ржпрж╝рж┐ржХрж╛',
            'ржмрж┐ржпрж╝рзЗ', 'ржмрж┐ржмрж╛рж╣', 'ржЕржирзБржкржо', 'ржХрж▓рзНржпрж╛ржгрзА', 'рж╢ржорзНржнрзБржирж╛рже',
            'ржорж╛ржорж╛', 'ржмрж╛ржмрж╛', 'ржорж╛', 'ржкрж┐рждрж╛', 'ржорж╛рждрж╛',
            'ржШржЯржирж╛', 'ржкрж░рж┐рж╕рзНржерж┐рждрж┐', 'рж╕ржВрж▓рж╛ржк', 'ржХржерзЛржкржХржержи',
            'рж╕рзНржЯрзЗрж╢ржи', 'ржЧрж╛ржбрж╝рж┐', 'ржЯрзНрж░рзЗржи', 'рж░рзЗрж▓', 'ржкрзНрж▓рзНржпрж╛ржЯржлрж░рзНржо',
            'ржпрж╛рждрзНрж░рж╛', 'ржнрзНрж░ржоржг', 'рж╕рзНржЯрзЗрж╢ржи-ржорж╛рж╕рзНржЯрж╛рж░', 'ржЯрж┐ржХрж┐ржЯ'
        ]
        
        # MCQ indicators (to exclude)
        mcq_indicators = [
            'ржкрзНрж░рж╢рзНржи', 'ржЙрждрзНрждрж░:', '(ржХ)', '(ржЦ)', '(ржЧ)', '(ржШ)',
            'рж╕ржарж┐ржХ', 'ржнрзБрж▓', 'ржирж┐ржЪрзЗрж░ ржХрзЛржиржЯрж┐', 'ржХрзЛржи рж╕рж╛рж▓рзЗ'
        ]
        
        # Check length (story content should be longer)
        if len(text.strip()) < 50:
            return False
            
        # Count indicators
        story_count = sum(1 for indicator in story_indicators if indicator in text)
        mcq_count = sum(1 for indicator in mcq_indicators if indicator in text)
        
        # If it has MCQ patterns, it's likely not story content
        if mcq_count > 0 and '(ржХ)' in text:
            return False
            
        # If it has story indicators and is substantial text, it's likely story
        # OR if it contains transportation/journey elements (train scenes)
        transportation_indicators = ['рж╕рзНржЯрзЗрж╢ржи', 'ржЧрж╛ржбрж╝рж┐', 'ржЯрзНрж░рзЗржи', 'рж░рзЗрж▓', 'ржкрзНрж▓рзНржпрж╛ржЯржлрж░рзНржо', 'ржпрж╛рждрзНрж░рж╛']
        has_transportation = any(indicator in text for indicator in transportation_indicators)
        
        return story_count > 0 or len(text.strip()) > 200 or has_transportation
    
    @staticmethod
    def clean_story_text(text: str) -> str:
        """Clean and normalize story text"""
        # Fix encoding first
        text = BengaliTextProcessor.fix_bengali_encoding(text)
        
        # Remove page numbers, headers, footers
        text = re.sub(r'ржкрзГрж╖рзНржарж╛\s*\d+', '', text)
        text = re.sub(r'Page\s*\d+', '', text)
        text = re.sub(r'^\d+\s*$', '', text, flags=re.MULTILINE)
        
        # Remove excessive whitespace but preserve paragraph breaks
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)  # Multiple empty lines -> double line break
        text = re.sub(r'[ \t]+', ' ', text)  # Multiple spaces/tabs -> single space
        text = text.strip()
        
        return text

class StoryFocusedProcessor:
    """Process PDF to extract and properly encode Bengali story content"""
    
    def __init__(self, pdf_path: str):
        self.pdf_path = pdf_path
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        
        if not self.google_api_key:
            raise ValueError("GOOGLE_API_KEY not found in environment variables")
    
    def extract_story_content(self) -> List[Dict]:
        """Extract story content from PDF with proper Bengali encoding"""
        print(f"ЁЯУЦ Extracting story content from: {self.pdf_path}")
        
        # Open PDF
        pdf_document = fitz.open(self.pdf_path)
        
        story_chunks = []
        page_count = 0
        story_pages = 0
        
        for page_num in range(pdf_document.page_count):
            page = pdf_document[page_num]
            
            # Extract text with better encoding handling
            raw_text = page.get_text(flags=fitz.TEXT_PRESERVE_LIGATURES | fitz.TEXT_PRESERVE_WHITESPACE)
            
            if not raw_text.strip():
                continue
                
            page_count += 1
            
            # Fix Bengali encoding
            fixed_text = BengaliTextProcessor.fix_bengali_encoding(raw_text)
            
            # Check if this is likely story content
            if BengaliTextProcessor.is_story_content(fixed_text):
                # Clean the story text
                clean_text = BengaliTextProcessor.clean_story_text(fixed_text)
                
                if len(clean_text.strip()) > 50:  # Only include substantial content
                    story_chunks.append({
                        "page_number": page_num + 1,
                        "content": clean_text,
                        "content_length": len(clean_text),
                        "content_type": "story"
                    })
                    story_pages += 1
                    print(f"тЬЕ Page {page_num + 1}: Extracted {len(clean_text)} chars of story content")
                else:
                    print(f"тЪая╕П  Page {page_num + 1}: Content too short after cleaning")
            else:
                print(f"тПня╕П  Page {page_num + 1}: Skipped (MCQ/metadata content)")
        
        pdf_document.close()
        
        print(f"\nЁЯУК Extraction Summary:")
        print(f"   Total pages processed: {page_count}")
        print(f"   Story content pages: {story_pages}")
        print(f"   Total story chunks: {len(story_chunks)}")
        
        return story_chunks
    
    def create_langchain_documents(self, story_chunks: List[Dict]) -> List[Document]:
        """Convert story chunks to LangChain documents with proper chunking"""
        print(f"ЁЯУД Creating LangChain documents from {len(story_chunks)} story chunks...")
        
        # Initialize text splitter for better chunking
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,  # Smaller chunks for better retrieval
            chunk_overlap=100,  # Some overlap to preserve context
            length_function=len,
            separators=['\n\n', '\n', 'ред', 'ред', '.', ' ']  # Bengali-aware separators
        )
        
        documents = []
        
        for chunk in story_chunks:
            page_num = chunk["page_number"]
            content = chunk["content"]
            
            # Split long content into smaller chunks
            if len(content) > 600:
                sub_chunks = text_splitter.split_text(content)
                
                for i, sub_chunk in enumerate(sub_chunks):
                    if len(sub_chunk.strip()) > 30:  # Only meaningful chunks
                        doc = Document(
                            page_content=sub_chunk.strip(),
                            metadata={
                                "page": page_num,
                                "chunk_id": f"{page_num}_{i}",
                                "content_type": "story",
                                "source": self.pdf_path,
                                "encoding_fixed": True
                            }
                        )
                        documents.append(doc)
            else:
                # Small content, keep as single document
                doc = Document(
                    page_content=content.strip(),
                    metadata={
                        "page": page_num,
                        "chunk_id": f"{page_num}_0",
                        "content_type": "story", 
                        "source": self.pdf_path,
                        "encoding_fixed": True
                    }
                )
                documents.append(doc)
        
        print(f"тЬЕ Created {len(documents)} LangChain documents")
        return documents
    
    def create_vector_store(self, documents: List[Document], persist_directory: str = "./chroma_db_story_focused") -> Chroma:
        """Create vector store with story-focused documents"""
        print(f"ЁЯФЧ Creating vector store at: {persist_directory}")
        
        # Remove existing vector store
        if os.path.exists(persist_directory):
            import shutil
            shutil.rmtree(persist_directory)
            print(f"ЁЯЧСя╕П  Removed existing vector store")
        
        # Initialize embeddings
        embeddings = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004",
            google_api_key=self.google_api_key
        )
        
        # Create vector store
        vectorstore = Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            persist_directory=persist_directory
        )
        
        # Persist the vector store
        vectorstore.persist()
        
        print(f"тЬЕ Vector store created with {len(documents)} documents")
        return vectorstore

def main():
    """Main function to process PDF and create story-focused vector store"""
    pdf_path = "./documents/HSC26-Bangla1st-Paper.pdf"
    
    if not os.path.exists(pdf_path):
        print(f"тЭМ PDF file not found: {pdf_path}")
        return
    
    print("ЁЯЪА Starting Story-Focused Processing...")
    print("=" * 60)
    
    try:
        # Initialize processor
        processor = StoryFocusedProcessor(pdf_path)
        
        # Extract story content with fixed encoding
        story_chunks = processor.extract_story_content()
        
        if not story_chunks:
            print("тЭМ No story content found in PDF")
            return
        
        # Create LangChain documents
        documents = processor.create_langchain_documents(story_chunks)
        
        if not documents:
            print("тЭМ No valid documents created")
            return
        
        # Create vector store
        vectorstore = processor.create_vector_store(documents)
        
        print("\nЁЯОЙ Story-focused processing completed successfully!")
        print(f"ЁЯУК Final Stats:")
        print(f"   Story chunks extracted: {len(story_chunks)}")
        print(f"   LangChain documents: {len(documents)}")
        print(f"   Vector store: chroma_db_story_focused")
        print(f"   Bengali encoding: тЬЕ Fixed")
        
        # Test the vector store
        print("\nЁЯзк Testing vector store...")
        test_results = vectorstore.similarity_search("ржЕржирзБржкржо ржХрж▓ржпрж╛ржгрзА", k=3)
        print(f"Test query returned {len(test_results)} results")
        
        for i, result in enumerate(test_results[:2]):
            print(f"\nResult {i+1}:")
            print(f"Page: {result.metadata.get('page', 'unknown')}")
            print(f"Content (first 100 chars): {result.page_content[:100]}...")
        
    except Exception as e:
        print(f"тЭМ Error during processing: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
